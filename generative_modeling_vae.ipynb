{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generative-modeling-vae.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dbhulwb6vFph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The Variational Autoencoder (VAE) introduced in the paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114), an extension of autoencoder neural network architecture, is used for generative modeling. It is widely being used to generate all kinds of art: text, images, audio, etc. In this notebook, we will learn how a VAE works by building and training one.\n",
        "\n",
        "## Autoencoder vs Variational Autoencoder\n",
        "An autoencoder learns to reconstruct the input image by forcing the model to learn it's dense and compact representation. This creates  a bottleneck in the information flow and forces the model to extract and learn the most important features which are necessary to reconstruct the image. Although this works, it does not gives us a continuous latent space. It just clusters the input data points in the latent space. \n",
        "\n",
        "### Latent space\n",
        "To generate samples, the latent space needs to be continous as we should be able to sample any point in the space and be able to get a realistic sample. If the space is continuous, we also get the benefit of finding different VARIATIONS of a given sample. For example, given a sample face, we could extract a new sample having the same face but with additional features like wearing a hat or sunglasses. Different directions in the latent space have different meanings and we can follow a vector from particular points to add those meanings to the points. This process is called Latent Space Interpolation.\n",
        "\n",
        "#### Making the latent space continuous\n",
        "To convert the discrete latent space learned by an autoencoder into continuous, we add stochasticity to it. Instead of learning fixed points representing the space, we learn the probability distributions from which we can sample the points. Instead of learning one latent space vector, we learn two different vectors which represent the mean and the standard deviation of the probability distribution of the latent space respectively. This gives us smooth regions around the training data.\n",
        "\n",
        "We still have two problems to address in this setup which we will discuss below as we proceed with building and training the network."
      ]
    }
  ]
}