{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generative-modeling-vae.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dbhulwb6vFph",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The Variational Autoencoder (VAE) introduced in the paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114), an extension of autoencoder neural network architecture, is used for generative modeling. It is widely being used to generate all kinds of art: text, images, audio, etc. In this notebook, we will learn how a VAE works by building and training one.\n",
        "\n",
        "## Autoencoder vs Variational Autoencoder\n",
        "An autoencoder learns to reconstruct the input image by forcing the model to learn it's dense and compact representation. This creates  a bottleneck in the information flow and forces the model to extract and learn the most important features which are necessary to reconstruct the image. Although this works, it does not gives us a continuous latent space. It just clusters the input data points in the latent space. \n",
        "\n",
        "### Latent space\n",
        "To generate samples, the latent space needs to be continous as we should be able to sample any point in the space and be able to get a realistic sample. If the space is continuous, we also get the benefit of finding different VARIATIONS of a given sample. For example, given a sample face, we could extract a new sample having the same face but with additional features like wearing a hat or sunglasses. Different directions in the latent space have different meanings and we can follow a vector from particular points to add those meanings to the points. This process is called Latent Space Interpolation.\n",
        "\n",
        "#### Making the latent space continuous\n",
        "To convert the discrete latent space learned by an autoencoder into continuous, we add stochasticity to it. Instead of learning fixed points representing the space, we learn the probability distributions from which we can sample the points. Instead of learning one latent space vector, we learn two different vectors which represent the mean and the standard deviation of the probability distribution of the latent space respectively. This gives us smooth regions around the training data.\n",
        "\n",
        "We still have two problems to address in this setup which we will discuss below as we proceed with building and training the network."
      ]
    },
    {
      "metadata": {
        "id": "8BZDmY_DF4c_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "We will use the [Fashion-MNIST](https://arxiv.org/abs/1708.07747) dataset provided by Zalando Research for training our VAE. It is inspired by MNIST dataset and has grayscale images of fashion articles. It has the same number of data samples, classes and image size but is more complex thant the MNIST dataset. We will import the dataset from the Keras Datasets API, add the channels dimension, which in our case is 1 since the images are in grayscale, and normalize it. We will ignore the labels as we do not need them for training our VAE."
      ]
    },
    {
      "metadata": {
        "id": "DHvCZNDJF7V6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "39884565-22ee-49bf-c1ff-3a8938790c91"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "x_train = (x_train.reshape(x_train.shape + (1,)))/255.\n",
        "x_test = (x_test.reshape(x_test.shape + (1,))) / 255.\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrLgEmbHGMao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the encoder model\n",
        "As discussed above, we will force the model to learn a probability distribution of the compact latent space so that it becomes continuous. We start with the image, extract features from it using convolutions, and learn two vectors that we will use to sample a point from latent space. We store the shape of the output of the last convolution layer, which we will use when we reconstruct our image back from the latent representation."
      ]
    },
    {
      "metadata": {
        "id": "jLEDAZmjGN7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras import Input\n",
        "from keras.layers import Conv2D, Flatten, Dense\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "hidden_state_size = 8\n",
        "\n",
        "x_input = Input(shape=(28, 28, 1), name='input_image')\n",
        "x = Conv2D(32, 3, padding='same', activation='relu')(x_input)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "shape = x.shape.as_list()[1:]\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "z_mean = Dense(hidden_state_size, name='z_mean')(x)\n",
        "z_log_variance = Dense(hidden_state_size, name='z_log_variance')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3R8t2qOGPX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "By adding stochasticity to the latent space, we force it to become continuous. But, we can only backpropagate our errors through deterministic nodes. To solve this, we apply a hack called the \"Reparameterization Trick\". Instead of making our latent space vector a stochastic node, we make it deterministic by adding a different stochastic node called \"epsilon\". The epsilon node simply samples values from a standard normal distribution, we multiply this epsilon with the standard deviation and add this term to the mean to get our latent space representation. The epsilon node is stochastic, but it is just a predefined distribution and we do not need to learn its parameters. This excludes it from the backpropagation and instead makes our latent space node deterministic. "
      ]
    },
    {
      "metadata": {
        "id": "lDMoI_HVGQ22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "afcb8c65-5e3f-4d10-9b78-11b25d895560"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Lambda\n",
        "def sampling(args):\n",
        "    z_mean, z_log_variance = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], hidden_state_size),\n",
        "                              mean=0., stddev=1.)\n",
        "    return z_mean + K.exp(z_log_variance) * epsilon\n",
        "z = Lambda(sampling, name='z')([z_mean, z_log_variance])\n",
        "encoder = Model(x_input, z)\n",
        "encoder.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_image[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 14, 14, 128)  73856       conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 14, 14, 128)  147584      conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 25088)        0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          3211392     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 8)            1032        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z_log_variance (Dense)          (None, 8)            1032        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 8)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_variance[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 3,453,712\n",
            "Trainable params: 3,453,712\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Nv0riPuGhgd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the decoder model\n",
        "We will build the decoder of our VAE as a separate model with its own input and output so that we can use it later with our own latent space representations and generate samples from it. We start with the shape that we saved earlier and apply reverse convolution with the layer Conv2DTranspose and then finally convolve on it to get an output image of the same shape as of our input image."
      ]
    },
    {
      "metadata": {
        "id": "GeeIvzuqGkc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "491ad5cd-238f-4330-eded-6bb62ec479ec"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "input_z = Input(z.shape.as_list()[1:], name='input_z')\n",
        "y = Dense(np.prod(shape), activation='relu')(input_z)\n",
        "y = Reshape(shape)(y)\n",
        "y = Conv2DTranspose(128, 3, padding='same', activation='relu', strides=(2, 2))(y)\n",
        "y = Conv2D(1, 3, padding='same', activation='sigmoid', name='output_image')(y)\n",
        "decoder = Model(input_z, y)\n",
        "y = decoder(z)\n",
        "decoder.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_z (InputLayer)         (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 25088)             225792    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 128)       147584    \n",
            "_________________________________________________________________\n",
            "output_image (Conv2D)        (None, 28, 28, 1)         1153      \n",
            "=================================================================\n",
            "Total params: 374,529\n",
            "Trainable params: 374,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cvzPvyB1G18W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Defining the loss\n",
        "By adding stochasticity to the latent space, we made the regions around the classes to be smooth. But, for latent space interpolation to work, we need to fill the gaps between these regions so that we can smoothly transition from one point in a class to another point in another class. We want to force all the regions of the different classes to coalesce into one big continuous space. We can do this by minizing the KL divergence between our latent space distribution and a standard normal distribution. This will force all of our latent space to come as close to the center and merge. Therefore, to train our VAE, we will optimize two losses: A reconstruction loss and KL-divergence loss. We use binary cross-entropy here as our reconstruction loss between the input and output image. "
      ]
    },
    {
      "metadata": {
        "id": "EotkUl6xG3Xu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "from keras.metrics import binary_crossentropy\n",
        "\n",
        "class VariationalLayer(Layer):\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        x, y = K.flatten(x), K.flatten(y)\n",
        "\n",
        "        ce_loss = binary_crossentropy(x, y)\n",
        "        kl_loss = 1 + z_log_variance - K.square(z_mean) - K.exp(z_log_variance)\n",
        "        kl_loss = -0.5*K.mean(kl_loss, axis=-1)\n",
        "        return K.mean(ce_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, y = inputs[0], inputs[1]\n",
        "        loss = self.loss(x, y)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return loss\n",
        "      \n",
        "y = VariationalLayer(name='loss')([x_input, y])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}